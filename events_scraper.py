#!/usr/bin/env python3
"""
Scraper √©v√©nements - Sources fiables
- LiA Le Havre pour gr√®ves transport
- Actualit√©s pour mouvements sociaux
- Jours f√©ri√©s officiels
"""
import requests
import json
from datetime import datetime, timedelta
from pathlib import Path
import re

def scrape_lia_disruptions(debug=True):
    """Scrape les perturbations LiA pertinentes (tramway ou gr√®ve) avec logs d√©taill√©s"""
    events = []

    try:
        import requests
        from bs4 import BeautifulSoup
        import re
        from datetime import datetime, timedelta

        url = "https://www.transports-lia.fr/fr/infos-trafic/17/Disruption"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

        if debug:
            print("üöè [LiA] T√©l√©chargement de la page des perturbations...")

        resp = requests.get(url, headers=headers, timeout=15)
        if resp.status_code != 200:
            print(f"‚ö†Ô∏è [LiA] Erreur HTTP {resp.status_code}")
            return events

        soup = BeautifulSoup(resp.text, "html.parser")

        # On r√©cup√®re les articles ou blocs de perturbations
        disruptions = soup.find_all("article") or soup.find_all("div", class_="disruption")
        if debug:
            print(f"üìã [LiA] {len(disruptions)} perturbation(s) trouv√©e(s) sur la page")

        months_map = {
            'janvier': 1, 'f√©vrier': 2, 'fevrier': 2, 'mars': 3, 'avril': 4,
            'mai': 5, 'juin': 6, 'juillet': 7, 'ao√ªt': 8, 'aout': 8,
            'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12, 'decembre': 12
        }

        kept = 0
        for i, item in enumerate(disruptions, 1):
            title = item.get_text(strip=True).lower()
            text = item.get_text(" ", strip=True).lower()

            # V√©rifie si c‚Äôest pertinent
            if not re.search(r'\b(tram|tramway|gr[e√®]ve)\b', title):
                if debug:
                    print(f"‚ùå Ignor√© #{i} (non pertinent) ‚Üí {title[:60]}...")
                continue

            # Extraction d‚Äôun titre lisible
            display_title = item.find("h3").get_text(strip=True) if item.find("h3") else title.title()

            # Recherche de dates dans le texte
            date_patterns = re.findall(
                r'(?:du |le |√† partir du )?(\d{1,2})\s+'
                r'(janvier|f√©vrier|fevrier|mars|avril|mai|juin|juillet|ao√ªt|aout|septembre|octobre|novembre|d√©cembre|decembre)',
                text
            )

            # Br√®ve description (30 premiers mots)
            description = " ".join(text.split()[:30]) + "..."
            event_dates = []

            if date_patterns:
                for day, month in date_patterns[:3]:
                    try:
                        month_num = months_map.get(month)
                        year = datetime.now().year
                        if month_num and month_num < datetime.now().month:
                            year += 1
                        d = datetime(year, month_num, int(day))
                        event_dates.append(d)
                    except Exception as e:
                        if debug:
                            print(f"‚ö†Ô∏è Erreur parsing date LiA : {e}")

            # Si aucune date trouv√©e, on suppose aujourd‚Äôhui / demain
            if not event_dates:
                event_dates = [datetime.now(), datetime.now() + timedelta(days=1)]

            for d in event_dates:
                if datetime.now() - timedelta(days=1) <= d <= datetime.now() + timedelta(days=14):
                    event = {
                        "date": d.strftime("%Y-%m-%d"),
                        "type": "greve" if "gr√®ve" in text or "greve" in text else "transport",
                        "title": display_title,
                        "description": description,
                        "source": "LiA Transports"
                    }
                    events.append(event)
                    kept += 1
                    if debug:
                        print(f"‚úÖ Gard√© #{i}: {display_title} ‚Üí {d.strftime('%d %b %Y')} ({event['type']})")
                else:
                    if debug:
                        print(f"‚è© Ignor√© #{i} (date trop lointaine) ‚Üí {display_title}")

        if debug:
            print(f"‚úÖ [LiA] {kept} perturbation(s) pertinente(s) gard√©e(s) sur {len(disruptions)}")

    except Exception as e:
        print(f"‚ö†Ô∏è [LiA] Erreur lors du scraping : {e}")

    return events


def scrape_greves(debug=True):
    """
    Scrape les infos de gr√®ves (France + Le Havre) depuis plusieurs sources m√©dias et open data.
    Ne garde que les gr√®ves futures, pertinentes et sourc√©es.
    """
    import requests
    from bs4 import BeautifulSoup
    import re
    from datetime import datetime, timedelta

    events = []
    today = datetime.now().date()

    sources = [
        ("https://www.service-public.fr/particuliers/actualites", "Service Public (officiel)"),
        ("https://www.francetvinfo.fr/economie/transports/sncf/", "France Info SNCF"),
        ("https://www.francetvinfo.fr/societe/greve/", "France Info Gr√®ves"),
        ("https://actu.fr/normandie/le-havre/", "Actu.fr Le Havre"),
    ]

    months_map = {
        'janvier': 1, 'f√©vrier': 2, 'fevrier': 2, 'mars': 3, 'avril': 4,
        'mai': 5, 'juin': 6, 'juillet': 7, 'ao√ªt': 8, 'aout': 8,
        'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12, 'decembre': 12
    }

    if debug:
        print("üì∞ [Gr√®ves] Scraping des sources d'actualit√©s...")

    for url, label in sources:
        try:
            resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
            if resp.status_code != 200:
                if debug:
                    print(f"‚ö†Ô∏è [{label}] Erreur HTTP {resp.status_code}")
                continue

            soup = BeautifulSoup(resp.text, "html.parser")
            links = soup.find_all("a", href=True)
            for a in links:
                text = a.get_text(" ", strip=True)
                if not text:
                    continue
                lower = text.lower()

                # üéØ Filtrage initial
                if not re.search(r"\b(gr[e√®]ve|mouvement social|mobilisation|perturbation|sncf|lia|transports?)\b", lower):
                    continue

                # ‚ùå Ignore les bilans ("bilan", "retard", "incendie", "manifest√©", etc.)
                if re.search(r"\b(bilan|retard|manifest√©|manifestants?|incendie|retards?|perturb√©|passagers|heures de retard)\b", lower):
                    continue

                # ‚öôÔ∏è Filtrage g√©ographique n√©gatif
                if re.search(r"\b(toulouse|bordeaux|lyon|marseille|nice|lille|strasbourg|nantes|montpellier|rennes)\b", lower):
                    if not re.search(r"\b(le\s*havre|normandie|national|toute la france|sncf|ratp)\b", lower):
                        continue

                href = a["href"]
                if not href.startswith("http"):
                    href = url.rstrip("/") + "/" + href.lstrip("/")

                # üîç On tente d'extraire plus d'infos dans l'article
                sub_desc = ""
                try:
                    sub_resp = requests.get(href, headers={"User-Agent": "Mozilla/5.0"}, timeout=8)
                    sub_soup = BeautifulSoup(sub_resp.text, "html.parser")
                    paragraphs = sub_soup.find_all("p")
                    if paragraphs:
                        sub_desc = " ".join(p.get_text(" ", strip=True) for p in paragraphs[:3])
                except:
                    pass

                # üß† Score de pertinence
                relevance = 0
                if re.search(r"\b(le\s*havre|normandie|lia|universit)\b", lower + sub_desc.lower()):
                    relevance += 2
                if re.search(r"\bnational|sncf|ratp|√©ducation|enseignants?\b", lower + sub_desc.lower()):
                    relevance += 1
                if relevance == 0:
                    continue

                # üîé Extraction de dates (futures uniquement)
                date_matches = re.findall(
                    r"(?:le|du|√† partir du)?\s*(\d{1,2})\s+(janvier|f√©vrier|fevrier|mars|avril|mai|juin|"
                    r"juillet|ao[u√ª]t|septembre|octobre|novembre|d[√©e]cembre)\b",
                    (lower + sub_desc.lower())
                )

                future_dates = []
                for day, month in date_matches[:3]:
                    try:
                        m = months_map.get(month.replace("√ª", "u"))
                        year = today.year
                        if m and m < today.month:
                            year += 1
                        d = datetime(year, m, int(day)).date()
                        # üî• On garde uniquement si la date est dans le futur (dans les 60 prochains jours)
                        if today <= d <= today + timedelta(days=60):
                            future_dates.append(d)
                    except Exception as e:
                        if debug:
                            print(f"‚ö†Ô∏è Erreur parsing date dans {label}: {e}")

                # üßπ Si pas de date future, on ignore
                if not future_dates:
                    continue

                # ‚úÖ Cr√©ation d'√©v√©nements
                for d in future_dates:
                    event = {
                        "date": d.strftime("%Y-%m-%d"),
                        "type": "greve",
                        "title": text.strip().capitalize(),
                        "description": (sub_desc[:300] + "...") if sub_desc else "Annonce de gr√®ve prochaine.",
                        "source": label,
                        "importance": relevance,
                        "link": href
                    }
                    events.append(event)
                    if debug:
                        print(f"‚úÖ [Gr√®ve] {label} ‚Üí {event['title']} ({event['date']})")

        except Exception as e:
            if debug:
                print(f"‚ö†Ô∏è Erreur scraping {label}: {e}")

    # üîß Nettoyage final : supprimer doublons + trier par date
    seen = set()
    filtered = []
    for ev in events:
        key = (ev['date'], ev['title'])
        if key not in seen:
            seen.add(key)
            filtered.append(ev)

    filtered = [e for e in filtered if datetime.strptime(e['date'], "%Y-%m-%d").year == today.year]
    filtered.sort(key=lambda x: x['date'])

    if debug:
        print(f"‚úÖ [Gr√®ves] {len(filtered)} gr√®ve(s) future(s) pertinente(s) conserv√©e(s).")

    return filtered


def get_jours_feries():
    """Jours f√©ri√©s fran√ßais officiels"""
    events = []
    current_year = datetime.now().year
    
    jours_feries_fixes = [
        (1, 1, "Jour de l'an"),
        (5, 1, "F√™te du Travail"),
        (5, 8, "Victoire 1945"),
        (7, 14, "F√™te Nationale"),
        (8, 15, "Assomption"),
        (11, 1, "Toussaint"),
        (11, 11, "Armistice 1918"),
        (12, 25, "No√´l")
    ]
    
    for year in [current_year, current_year + 1]:
        for month, day, nom in jours_feries_fixes:
            date = datetime(year, month, day)
            if datetime.now() <= date <= datetime.now() + timedelta(days=30):
                events.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'type': 'ferie',
                    'title': nom,
                    'description': 'Jour f√©ri√© - Services publics ferm√©s',
                    'source': 'Calendrier officiel'
                })
    
    # Jours mobiles
    paques_dates = {
        2024: datetime(2024, 3, 31),
        2025: datetime(2025, 4, 20),
        2026: datetime(2026, 4, 5)
    }
    
    for year, paques in paques_dates.items():
        lundi_paques = paques + timedelta(days=1)
        ascension = paques + timedelta(days=39)
        pentecote = paques + timedelta(days=50)
        
        for date, nom in [(lundi_paques, "Lundi de P√¢ques"), 
                          (ascension, "Ascension"), 
                          (pentecote, "Lundi de Pentec√¥te")]:
            if datetime.now() <= date <= datetime.now() + timedelta(days=30):
                events.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'type': 'ferie',
                    'title': nom,
                    'description': 'Jour f√©ri√©',
                    'source': 'Calendrier officiel'
                })
    
    return events

def merge_and_deduplicate(events):
    """Fusionne et d√©duplique"""
    seen = set()
    unique = []
    
    for event in events:
        key = (event['date'], event['title'])
        if key not in seen:
            seen.add(key)
            unique.append(event)
    
    unique.sort(key=lambda x: x['date'])
    return unique

def save_events(events):
    """Sauvegarde dans planned_events.json"""
    output = {
        'last_update': datetime.now().isoformat(),
        'events': events
    }
    
    with open('planned_events.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Sauvegard√© {len(events)} √©v√©nements")

def main():
    print("="*60)
    print("SCRAPER √âV√âNEMENTS")
    print(f"Ex√©cution: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
    print("="*60)
    
    all_events = []
    
    print("\nüìÖ Jours f√©ri√©s...")
    feries = get_jours_feries()
    all_events.extend(feries)
    print(f"  ‚úì {len(feries)} jours f√©ri√©s")
    
    print("\nüöå Perturbations LiA...")
    lia = scrape_lia_disruptions()
    all_events.extend(lia)
    print(f"  ‚úì {len(lia)} perturbations LiA")
    
    print("\nüì∞ Mouvements sociaux...")
    greves = scrape_greves()
    all_events.extend(greves)
    print(f"  ‚úì {len(greves)} mouvements sociaux")
    
    unique = merge_and_deduplicate(all_events)
    save_events(unique)
    
    print(f"\n‚úÖ TOTAL: {len(unique)} √©v√©nements")
    
    if unique:
        print("\nüìã Prochains √©v√©nements:")
        for event in unique[:5]:
            print(f"  ‚Ä¢ {event['date']} - {event['title']}")


if __name__ == "__main__":
    main()
